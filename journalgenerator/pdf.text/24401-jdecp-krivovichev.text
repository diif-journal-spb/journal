dx
dt
6
 
-
?
ДИФФЕРЕНЦИАЛЬНЫЕ УРАВНЕНИЯ
И
ПРОЦЕССЫ УПРАВЛЕНИЯ
N. 4, 2024
Электронный журнал,
рег. Эл. N ФС77-39410 от 15.04.2010
ISSN 1817-2172
http://diffjournal.spbu.ru/
e-mail: jodiff@mail.ru
Численные методы
Применение модифицированного метода Рунге — Кутты к
построению метода спуска для решения краевых задач
Г. В. Кривовичев, Н. В. Егоров
Факультет прикладной математики - процессов управления
Санкт-Петербургского государственного университета
E-mail: g.krivovichev@spbu.ru, n.v.egorov@spbu.ru
Аннотация. Работа посвящена построению и анализу градиентного ме-
тода, основанного на модифицированном явном методе Рунге — Кутты вто-
рого порядка, построенном с использованием разложения Лагранжа — Бюр-
мана. Предложен двухшаговый метод с инерцией, основанный на методе тя-
желого шарика. Доказаны теоремы о сходимости для сильно выпуклой квад-
ратичной и возмущенной квадратичной функции. Получены аналитические
выражения для параметров метода, обеспечивающие оптимальную скорость
сходимости. В случае квадратичной функции показано, что предложенный
метод сходится быстрее, чем другие ускоренные методы.
Представлены результаты применения метода к численному решению
линейных и нелинейных краевых задач: задачи Дирихле для трехмерного
уравнения Пуассона, задачи вариационного исчисления, задач для интегро-
дифференциальных уравнений. Показано, что по сравнению с известными
методами, предложенный метод позволяет получать численное решение с
нужной точностью при разных разбиениях сетки за меньшее число итера-
ций и время.Дифференциальные уравнения и процессы управления,N. 4, 2024
Ключевые слова: методы Рунге — Кутты, выпуклая оптимизация, гра-
диентный спуск, краевые задачи.
1 Введение
В настоящее время для решения задач оптимизации активно используются
методы, разработанные для решения задачи Коши для обыкновенных диф-
ференциальных уравнений (ОДУ) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]. В частности,
ряд градиентных методов построен при применении методов Рунге — Кутты
(МРК).
ПрименениюМРКкрешениюразныхзадачоптимизациипосвященомно-
го работ. В [12] описано применение явных МРК к решению задачи миними-
зации, эквивалентной решению системы нелинейных алгебраических уравне-
ний. В [13] показано, что использование МРК при решении таких задач при-
водит к ускорению сходимости по сравнению с методом градиентного спуска.
В [14] на примерах разных задач безусловной оптимизации проведен сравни-
тельный анализ градиентных методов на основе явных и неявных МРК. В
[15] неявные МРК низких порядков точности используются для построения
квазиньютоновских методов. Работа [2] посвящена построению предобуслов-
ленных градиентных методов на основе явных и неявных МРК. Авторами
показано, что ограничения на шаг, обеспечивающие сходимость, связаны с
условиями устойчивости используемых МРК. Также показано, что такие ме-
тоды можно использовать для построения преобуславливателей для метода
сопряженныхградиентов.В[16]авторамипредложеноОДУвторогопорядка,
эквивалентное методу Нестерова и проведено исследование свойств его реше-
ний.В[5,6]построенамодификацияэтогоОДУиспомощьюметодафункций
Ляпуновапоказано,чтоприменениеявныхМРКкрешениюзадачиКошидля
такого уравнения приводит к ускорению сходимости при применении метода
к специальному классу функций. В [8] представлена динамическая система
(т.н. Nesterov gradient flow), построенная как база для ускоренных методов
на основе A-устойчивых неявных МРК. В [9, 17] для построения ускоренных
методов используются симплектические интеграторы. B [3] предложены гра-
диентные методы на основе методов РК – Чебышева, ориентированных на
решение жестких задач Коши. Методы стохастического градиентного спус-
ка на основе таких схем предложены в [4]. В [11] предложены градиентные
методы на основе явной схемы Ньюмарка, широко используемой для инте-
грирования по времени уравнений механики деформируемого твердого тела.
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 2Дифференциальные уравнения и процессы управления,N. 4, 2024
В работе показано, что полученный метод эквивалентен методу Нестерова.
ВпоследниедесятилетиядляулучшениясвойствМРК(устойчивости,мо-
нотонности, сокращения числа стадий)предложен ряд подходов (например,
см. [3, 18, 19, 20, 21, 22]). Один из наиболее интересных подходов связан с
построением методов на основе разложений локальной погрешности по фор-
мулам, отличным от формулы Тейлора. В [23, 24] Е.В. Ворожцовым были
построены явные МРК на основе разложения Лагранжа — Бюрмана (ЛБ),
использующего степени нелинейной функции от шага интегрирования. В [23]
были построены одно- и двухстадийные явные схемы и было показано, что
области устойчивости таких методов могут иметь б´ ольшие площади по срав-
нению с стандартными методами. В [25, 26] показано, что с помощью таких
разложений можно строить монотонные схемы высокого порядка аппрокси-
мации для гиперболических систем уравнений в частных производных. Не
зависимо от этих исследований, МРК на основе разложений ЛБ были пред-
ложены в [27].
С учетом описанной в [2] близости между условями сходимости гради-
ентных методов и условиями устойчивости МРК, использование МРК с воз-
можностью увеличения площади области устойчивости для построения гра-
диентных методов является весьма перспективным.
Настоящая работа посвящена построению и анализу сходимости гради-
ентного метода на основе МРК, использующего разложения ЛБ. Предложен
предобусловленныйметодсинерцией,основанныйнакомбинацииидей,пред-
ложенных в работах [2, 28]. Доказана теорема о сходимости и получены фор-
мулы для оптимального шага и скорости сходимости в случае квадратичной
и возмущенной квадратичной функции. Метод применяется к численному ре-
шениюкраевыхзадачдлядифференциальныхиинтегро-дифференциальных
уравнений. Показано, что метод позволяет находить решение за меньшее чис-
ло итераций и время по сравнению с известными градиентными методами.
2 Метод спуска на основе метода Рунге — Кутты вто-
рого порядка с разложением Лагранжа — Бюрмана
Рассмотрим функцию f : R
d
→ R, являющуся сильно выпуклой с констан-
той выпуклости l > 0, градиент которой удовлетворяет условию Липшица с
константой L>0. Рассмотрим задачу безусловной оптимизации:
f(x)→ min
x∈R
d
, x
∗ = argmin
x∈R
d
f(x). (1)
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 3Дифференциальные уравнения и процессы управления,N. 4, 2024
Одним из наиболее известных методов решения такой задачи является метод
градиентного спуска (gradient descent, GD):
x
k+1
=x
k
− h∇f(x
k
), k =0,1,..., (2)
где h > 0. Непрерывным аналогом этого метода является задача Коши для
системы вида:
˙ x=−∇ f(x). (3)
Как известно [1, 2], положение равновесия этой системы совпадает с x
∗ . С
учетом этого, методы решения начальной задачи для (3) можно трактовать
как методы решения задачи (1), что и используется при построении новых
градиентных методов.
2.1 Явные методы Рунге — Кутты на основе разложений Лагран-
жа — Бюрмана
Рассмотрим задачу Коши для автономной системы ОДУ:
˙ x=g(x), x(t
0
)=x
0
, (4)
где g :R
d
→R будем считать достаточно гладкой.
Рассмотрим бесконечно дифференцируемую функию y(t) и пусть функ-
ция z = s(t), z
0
= s(t
0
), ˙ s(t
0
) ̸= 0 является достаточно гладкой. Формула
разложения ЛБ имеет следующий вид [23, 29]:
y(t)=y(t
0
)+
∞
X
k=1
(z− z
0
)
k
k!
 
d
k− 1
dt
k− 1
"
˙ y(t)
 
t− t
0
s(t)− z
0
 
k
#!
t=t
0
. (5)
Как можно видеть, при s(t)=t,z
0
=t
0
формула (5) приводит к разложению
в ряд Тейлора. Вводя функию φ, такую что φ(t− t
0
) = s(t)− z
0
и требуя,
чтобы φ(0)=0, ˙ φ(0)̸=0, получим, что (5) может быть переписана в виде:
y(t)=y(t
0
)+
∞
X
k=1
b
k
[φ(t− t
0
)]
k
, (6)
где коэффициенты b
k
вычисляются как [23]:
b
k
= lim
t→t
0
1
k!
d
k− 1
dt
k− 1
"
˙ y(t)
 
t− t
0
φ(t− t
0
)
 
k
#
, k =1,2,...
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 4Дифференциальные уравнения и процессы управления,N. 4, 2024
Выражения для b
i
при i=1,4 представлены в [23]. Как можно видеть, фор-
мула (6) представляет собой разложение y(t) в ряд по степеням φ(t− t
0
).
Явный МРК, основанный на разложении ЛБ, имеет следующий вид:
x
k+1
=x
k
+
q
X
i=1
p
i
K
i
(h),
где x
k
≈ x(t
k
), t
k
— узлы сетки, построенной с шагом h,
K
1
(h)=φ(h)g(x
k
), K
i
(h)=φ(h)g
 
x
k
+
i− 1
X
j=1
β ij
K
j
(h)
!
, i=2,q,
гдеp
i
,β ij
яляютсяпараметрамиметода.Этипараметрынаходятсяизусловий
порядка, как в случае обычных МРК, но при этом используется разложение
(6) локальной погрешности x(t
k+1
)− x
k+1
в окрестности точки h=0.
В [23] построен явный метод второго порядка следующего вида:
x
k+1
=x
k
+
K
1
+3K
2
4˙ φ(0)
, K
1
=φ(h)g(x
k
), K
2
=φ(h)g
 
x
k
+
2K
1
3˙ φ(0)
 
. (7)
Как показано в [23, 25, 26], посредством выбора функции φ(h) можно постро-
ить монотонные схемы для численного решения (4), что важно при решении
жестких задач Коши и задач с разрывными решениями. Метод (7) имеет
следующий полином устойчивости:
R(z)=1+γz +
γ 2
z
2
2
, (8)
где γ =
φ(h)
h˙ φ(0)
. Отметим, что обычный МРК второго порядка соответствует
значению γ = 1. Как можно видеть из (8), на область устойчивости такого
метода можно влиять посредством варьирования значений γ > 0.
Для улучшения точности (7) в [23] предложено использовать модифика-
цию этого метода. При нечетной φ получим, чтоφ(h)= ˙ φ(0)h+O(h
3
). Тогда
˙ φ(0)=
φ(h)
h
+O(h
2
). Используя приближение ˙ φ(0)≈ φ(h)
h
только в формуле
для расчета x
k+1
, получим метод вида:
x
k+1
=x
k
+
h(K
1
+3K
2
)
4φ(h)
, (9)
K
1
=φ(h)g(x
k
), K
2
=φ(h)g
 
x
k
+
2K
1
3˙ φ(0)
 
.
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 5Дифференциальные уравнения и процессы управления,N. 4, 2024
Как отмечается в [23], этот метод тоже имеет второй порядок точности. При
этом показано, что при решении задач с известным точным решением он дает
меньшую погрешность, чем исходный метод. Полином устойчивости метода
(9) имеет вид:
R(z)=1+z+
γz
2
2
.
При применении метода (9) к задаче для линейной системы с g(x) =
Px+c, гдеdim(P)=d× d,dim(c)=d, получается следующий итерационный
метод:
x
k+1
=
 
E +hP +
γh
2
2
P
2
 
x
k
+h
 
E +
hγ
2
P
 
c,
где E есть единичная матрица. Таким образом, в случае линейной системы
нет необходимости указывать конкретный вид функции φ(h) и метод опре-
деляется значением параметра γ , при котором он должен быть устойчив.
2.2 Случай квадратичной функции
Рассмотрим функцию следующего вида:
f(x)=
1
2
(x,Ax)− (b,x),
где b ∈ R
d
, A — положительно определенная симметричная матрица с соб-
ственными знаечниями 0 < l = λ 1
≤ λ 2
≤ ... ≤ λ d
= L. Градиент этой
функции записывается как∇f(x)=Ax− b. Система (3) примет следующий
вид:
˙ x=− (Ax− b), (10)
и точка минимума f(x) является решением линейной системы Ax=b.
Применяя (9) к задаче Коши для (10), получим метод следующего вида:
x
k+1
=x
k
− hD∇f(x
k
), (11)
где матрица D =E− γh
2
A может рассматриваться как предобуславливатель
для метода градиентного спуска.
Один из подходов к ускорению сходимости метода (2) заключается в до-
бавлении инерционного слагаемого, что приводит к т.н. методу тяжелого ша-
рика (heavy ball method, HB):
x
k+1
=x
k
− h∇f(x
k
)+β (x
k
− x
k− 1
), (12)
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 6Дифференциальные уравнения и процессы управления,N. 4, 2024
где β ∈[0,1).
По аналогии с (12), добавим в (11) инерционный член:
x
k+1
=x
k
− hD∇f(x
k
)+β (x
k
− x
k− 1
). (13)
Для обозначения метода (13) в дальнешем будем использовать аббревиатуру
LBHB (Lagrange — Burmann heavy ball).
Можно сформулировать следующую теорему о сходимости метода (13):
Теорема 1. Пусть шаг h>0, параметры γ > 0, β ∈[0,1) и число обуслов-
ленности κ =
L
l
≥ 14 удовлетворяют следующим условиям:
h<
2
γL
; (14)
β ≥  
1− p
ψ (h,λ ;γ )
 
2
, ∀λ ∈[l,L], (15)
где ψ (h,λ ;γ )=hλ
 
1− γ 2
hλ
 
;
γ >
1
4
 
√
2κ 1+κ +
1
√
2
!
2
. (16)
Тогда: метод (13) линейно сходится к x
∗ , при этом оптимальная ско-
рость сходимости имеет вид:
ρ opt
=1− r
2
γ √
κ 1+κ , (17)
и достигается при следующих значениях h и β :
h
opt
=
2
γ (l+L)
, β opt
=
 
1− r
2
γ √
κ 1+κ  
2
. (18)
Доказательство. Отметим, что выполнение (14) гарантирует, что ψ > 0 и
допустимо рассматривать квадратный корень от этой величины.
1) Получим ограничение на γ , которое обеспечивает выполнение следую-
щего неравенства:
− 1<1− p
ψ < 1, ∀λ. (19)
Выполнение правой части этого неравенства очевидно. Левая часть приводит
к неравенству 2− √
ψ > 0, которое эквивалентно ψ < 4. Последнее неравен-
ство можно переписать в виде:
γ 2
t
2
− t+4>0, (20)
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 7Дифференциальные уравнения и процессы управления,N. 4, 2024
где t=hλ . Дискриминант соответствующего уравнения равен 1− 8γ . Таким
образом, при выполнении условия
γ >
1
8
, (21)
неравенство (20) будет верно для всех λ ∈[l,L] и h, определяемого по (14).
Отметим, что условие (16) согласуется с (21). Это очевидно из неравен-
ства:
1
4
 
√
2κ 1+κ +
1
√
2
!
2
>
1
8
. (22)
Таким образом, выбирая γ по (16), получим, что будет справедливо неравен-
ство (19).
2) С использованием вектора z
k
=(x
k
− x
∗ ,x
k− 1
− x
∗ )
T
метод (13) может
быть быть представлен в виде:
z
k+1
=Tz
k
,
где матрица T имеет вид:
T =
 
(1+β )E− hDA − βE
E 0
d× d
!
.
Как известно [30], необходимое и достаточное условие сходимости такого ме-
тода имеет вид r(T)<1, где r(T) есть спектральный радиус матрицы T.
Представим A через спектральное разложение: A = SΛ S
T
, где Λ есть
диагональная матрица собственных значений A, а S — матрица собственных
векторов, SS
T
=S
T
S =E. Используя S, построим матрицу T =Σ T
TΣ , где
Σ=
 
S 0
d× d
0
d× d
S
!
, T =
 
(1+β )E− Ψ( h,Λ; γ ) − βE
E 0
d× d
!
,
гдеΨ( h,Λ; γ )=h(E− γh
2
Λ)Λ .Отметим,чтоматрицыT иT имеютодинаковые
собственные значения.
Покажем, что собственные значения T совпадают с собственными значе-
ниями матрицы вида:
˜
T =






T
1
0
2× 2
... 0
2× 2
0
2× 2
T
2
... 0
2× 2
... ... ... ...
0
2× 2
0
2× 2
... T
d






,
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 8Дифференциальные уравнения и процессы управления,N. 4, 2024
где T
i
есть матрицы размерности 2× 2, имеющие вид
T
i
=
 
1+β − ψ (h,λ i
;γ ) − β 1 0
!
.
Матрица T − ζE имеет следующий вид:
T − ζE =
 
T
11
T
12
T
21
T
22
!
,
где T
11
= (1+β )E− Ψ( h,Λ; γ )− ζE , T
12
= − βE , T
21
= E, T
22
= − ζE . Ее
определитель вычисляется по следующей формуле [31]:
det(T − ζE )=det(T
11
)det(T
22
− T
21
T
− 1
11
T
12
)=
=det(T
11
)det










− ζ +
β η 1
0 ... 0
0 − ζ +
β η 2
... 0
... ... ... ...
0 0 ... − ζ +
β η d










=
=(β − ζη 1
)(β − ζη 2
)...(β − ζη d
),
где η i
=1+β − ψ (h,λ i
;γ )− ζ , i=1,d.
В свою очередь, определитель блочно-диагональной матрицы
˜
T− ζE вы-
числяется как:
det(
˜
T − ζE )=det(T
1
− ζE
2× 2
)det(T
2
− ζE
2× 2
)...det(T
d
− ζE
2× 2
),
и совпадает сdet(T− ζE ). Таким образом, матрицыT и
˜
T имеют одинаковые
собственные значения ζ k
, k = 1,2d, которые вычисляются как собственные
значения матриц T
i
.
3) При выполнении условия (15) собственные значения T лежат внутри
единичного круга. Собственные значения являются корнями уравнения:
ζ 2
− (1+β − ψ )ζ +β =0,
которые имеют вид
ζ 1,2
=
1
2
(1+β − ψ ± √
D),
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 9Дифференциальные уравнения и процессы управления,N. 4, 2024
гдеD =(1+β − ψ )
2
− 4β . Нетрудно показать, что при выполнении условия
(15) D ≤ 0. Модули корней имеют вид |ζ 1,2
| =
√
β , и в соответствии с тем,
что β < 1, получаем, что (13) сходится к x
∗ .
Линейная сходимость метода (13) следует из теоремы 1 §1 Главы 2 из [32].
Скорость сходимости определяется спектральным радиусом r(T)=
√
β :
||x
k
− x
∗ ||≤ (
p
β +ε
k
)
k
||x
0
− x
∗ ||,
где ε
k
→0, k→∞.
4) Рассмотрим функцию G(t) =
 
1− q
t
 
1− γt
2
 
 
2
, t = λh > 0, где h
считается фиксированным. С учетом условия (14) и положительности λ она
определена на интервале
 
0,
2
γ  
. Знак ее первой производной
G
′
(t)=− 
1− s
t
 
1− γt
2
 
!
1− γt
q
t
 
1− γt
2
 
,
определяется знаками функций 1− γt и η (t)=1− q
t
 
1− γt
2
 
.
Исследуем поведение функции η (t): с учетом знака η ′
(t) она убывает при
t <
1
γ и возрастает при t >
1
γ . Интервал, в котором η (t) > 0, определяется
неравенством следующего вида:
r
t− γt
2
2
<1⇔γt
2
− 2t+2>0.
Дискриминант соответствующего уравнения равен 4− 8γ , так что при γ >
1
2
η (t) строго положительна и при выполнении этого ограничения на γ произ-
водная G
′
(t) меняет знак только в точке t=
1
γ .
При γ <
1
2
η ′
(t) обращается в нуль в двух точках:
t
1,2
=
1± √
1− 2γ γ .
Несложно увидеть, что t
1,2
∈
 
0,
2
γ  
.
Таким образом, приγ ≥ 1
2
G(t) убывает приt<
1
γ и возрастает приt>
1
γ ,
ее минимум достигается в точке t =
1
γ а максимальное значение достигается
на границе интервала определения. При γ ∈
 
1
8
,
1
2
 
функция G(t) убывает
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 10Дифференциальные уравнения и процессы управления,N. 4, 2024
при t < t
1
и t∈
 
1
γ ,t
2
 
, а возрастает при t∈
 
t
1
,
1
γ  
и t > t
2
. В этом случае
максимальные значения могут достигаться при t =
1
γ и в граничных точках
промежутка. Очевидно, что подобное поведение будет место и на промежутке
[lh,Lh]⊂  
0,
2
γ  
.
5) Предположим, что G(t) имеет наибольшее значение в граничной точке
t = lh или t = Lh, при этом значение η (t) строго положительно (как будет
показано ниже, последнее будет справедливо при выполнении (16)).
С учетом этого предположения, оптимальное значение h можно получить
как решение следующей задачи минимизации функции вида:
β opt
=min
h
max(χ 1
(h),χ 2
(h)),
где χ 1
(h) =
 
1− p
ψ (h,l;γ )
 
2
, χ 2
(h) =
 
1− p
ψ (h,L;γ )
 
2
. С учетом отме-
ченных выше свойств, оптимальное значение h соответствует точке пересе-
чения графиков этих функций:
 
1− s
hl
 
1− γ (lh)
2
2
 
!
2
=
 
1− s
hL
 
1− γ (Lh)
2
2
 
!
2
. (23)
Решение этого уравнения имеет вид:
h
opt
=
2
γ (l+L)
.
Оптимальное значение β имеет вид:
β opt
=
 
1− r
2
γ √
κ 1+κ  
2
,
а оптимальная скорость сходимости определяется как ρ opt
=
p
β opt
.
6) Покажем, что при κ ≥ 14 и выполнении (16) значение h, определяемое
из(23),отвечаетстрогоположительномузначению1− r
2
γ √
κ 1+κ .Неравенство
1− r
2
γ √
κ 1+κ >0
эквивалентно следующему ограничению на γ :
γ >
2κ (1+κ )
2
. (24)
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 11Дифференциальные уравнения и процессы управления,N. 4, 2024
Покажем, что
2κ (1+κ )
2
<
1
8
,
при κ ≥ 14. Это неравенство эквивалентно κ 2
− 14κ +1>0, и как несложно
увидеть, выполняется приκ> 7+
√
45≈ 13.71. Таким образом, с учетом (22)
условие (24) справедливо для значений γ , определяемых из (16) и при таком
условии оптимальное значение h получается как решение уравнения (23).
7)ЛокальныймаксимумG(t)вточкеt=
1
γ равен
 
1− 1
√
2γ  
2
.Покажем,
что при выполнении (16) справедливо следующее неравенство:
max(χ 1
(h),χ 2
(h))>
 
1− 1
√
2γ  
2
, ∀h∈
 
0,
2
γL
 
. (25)
Как было показано ранее, (25) при выполнении (16) сводится к неравенству:
 
1− r
2
γ √
κ 1+κ  
2
>
 
1− 1
√
2γ  
2
,
которое может быть переписано в виде:
(1− aτ )
2
>(1− bτ )
2
,
где τ =
1
√
γ , a =
√
2κ 1+κ , b =
1
√
2
. Последнее неравенство эквивалентно нера-
венству:
(a
2
− b
2
)τ > 2(a− b). (26)
В соответствии с
2
√
κ< 1+κ ⇔(1− √
κ )
2
>0,
можно видеть, что при κ > 1 в (26) a < b. Таким образом, (26) принимает
вид:
a+b
2
<
1
τ .
Это неравенство следует из (16).
Замечание.Припрактическомпримененииметода(13)γ рассматривается
как входной параметр, на значения которого наложено условие вида (16).
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 12Дифференциальные уравнения и процессы управления,N. 4, 2024
Рассмотрим функцию следующего вида:
c(κ )=
1
4
 
√
2κ 1+κ +
1
√
2
!
2
.
Как было показано ранее, значения этой функции больше
1
8
(см. (22)). Про-
изводная этой функции имеет вид:
c
′
(κ )=
1
4
(1+
√
k)
2
(1− κ )
√
κ (1+κ )
3
.
Как можно видеть, она является отрицательной при κ ≥ 14. Таким образом,
c(κ ) убывает и ее наибольшее значениеc(14)≈ 0.281. При этомc(κ )→
1
8
при
κ →∞.
Как можно видеть из выражения для h
opt
(см. (18)), большие значения
шага h получаются при уменьшении значений γ . Из (17) в этом случае полу-
чим малые значенияρ opt
, что приводит к ускорению сходимости. Таким обра-
зом, можно ожидать, что при б´ ольшем шаге будем получать меньшее число
итераций и время расчетов, необходимое для получения решения с заданной
точностью. Таким образом, при проведении расчетов при фиксированном κ можно предложить следующий способ выбора γ :
γ =c(κ )+eps, (27)
где eps > 0 является достаточно малым для того, чтобы избежать влияния
погрешностей округления.
2.3 Случай возмущенной квадратичной функции
Рассмотрим функцию вида [3]:
f(x)=
1
2
(x,Ax)+g(x), (28)
где A есть положительно определенная симметричная матрица, собственные
значения которой принадлежат промежутку [l,L], градиент g(x) удовлетво-
ряет условию Липшица:
∃M >0, ∀x,y∈R
d
:||∇g(x)−∇ g(y)||≤ M||x− y||,
а ее старшие производные являются малыми:
||∇
(i)
g(x)||≪ 1, i=2,3,...
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 13Дифференциальные уравнения и процессы управления,N. 4, 2024
В силу последнего условия, матрица A доминирует в матрице Гессе для f(x):
∇
2
f(x) ≈ A. По аналогии с [3], построим метод, в рамках которого можно
линеаризовать g(x) в окрестности x
k
: g(x) ≈ g(x
k
)+(∇g(x
k
),x− x
k
). При
использовании такого приближения градиент f(x) для всехx из этой окрест-
ности вычисляется как:
∇f(x)=Ax+∇g(x
k
). (29)
Применяя метод второго порядка (9) к задаче (3) в случае, когда f(x)
представима в виде (28), получим:
x
k+1
=x
k
− h
4φ(h)
(K
1
+3K
2
),
где
K
1
=− φ(h)∇f(x
k
)=− φ(h)
 
Ax
k
+∇g(x
k
)
 
,
K
2
=− φ(h)∇f
 
x
k
+
2K
1
3˙ φ(0)
 
=− φ(h)A
 
x
k
+
2K
1
3˙ φ(0)
 
− φ(h)∇g(x
k
).
Таким образом, получим метод:
x
k+1
=
 
E− hA+
hφ(h)
2˙ φ(0)
A
2
 
x
k
+h
 
φ(h)
2˙ φ(0)
A− E
 
∇g(x
k
),
который с использованием выражения для γ примет вид:
x
k+1
=Bx
k
− hD∇g(x
k
), (30)
где матрицы B и D определяются аналогично случаю квадратичной f(x).
Сформулируем следующую теорему о сходимости метода (30):
Теорема 2. Пусть M >0 удовлетворяет неравенству:
8M
L
<1,
где h>0, γ > 0 такие, что:
h<
2
γL
, γ >
1
4
, ρ ∈
 
0,1− 8M
L
 
, (31)
гдеρ =||B||=max
i
 
 
 1− hλ
i
+
γh
2
2
λ 2
i
 
 
 .Тогда:метод (30)сходитсяприлюбом
x
0
и справедливо следующее неравенство:
||x
k
− x
∗ ||≤  
ρ +
8M
L
 
k
||x
0
− x
∗ ||. (32)
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 14Дифференциальные уравнения и процессы управления,N. 4, 2024
Доказательство. Учитывая, что ∇f(x
∗ ) = 0 ⇔ Ax
∗ +∇g(x
∗ ) = 0, метод
(30) можно переписать в виде:
x
k+1
− x
∗ =Bx
k
− hD∇g(x
k
)− x
∗ ± Bx
∗ +hD(Ax
∗ +∇g(x
∗ )), (33)
Используя выражения
B− E =− hA+
γh
2
2
A
2
, hDA=hA− γh
2
2
A
2
,
из (33) получим:
||x
k+1
− x
∗ ||≤|| B(x
k
− x
∗ )||+h||D(∇g(x
k
)−∇ g(x
∗ ))||.
Покажем, что при выполнении (31) матрица B такая, что ||B|| < 1. В
силу симметричности этой матрицы, ее собственные значения имеют вид 1− hλ
i
+
γh
2
2
λ 2
i
, в связи с чем это условие можно переписать в виде:
− 1<1− λ i
h+
γ 2
λ 2
i
h
2
<1, ∀i. (34)
Его правая часть имеет вид:− hλ
i
 
1− γh
2
λ i
 
<0, и в связи с положительно-
стью λ i
выполняется при всех i в силу h <
2
γL
. Левая часть (34) принимает
вид:
2− λ i
h+
γ 2
λ 2
i
h
2
>0.
Соответствующий дискриминант равен 1− 4γ , в связи с чем оно выполняется
для всехi в силу (31). Кроме того, в силу1− γh
2
λ i
>0 получим, что||D||<1.
Учитывая отмеченные свойства матриц B и D, получим:
||x
k+1
− x
∗ ||≤ (ρ +hM)||x
k
− x
∗ ||<
 
ρ +
2
γL
M
 
||x
k
− x
∗ ||<
 
ρ +
8M
L
 
||x
k
− x
∗ ||.
С учетом (31) ρ +
8M
L
< 1 и неравенство (32) получается итерированием
последнего неравенства.
Замечание. С учетом приближения∇
2
f(x)≈ A, метод (13) при практи-
ческом применении к минимизации (28) можно применять при параметрах,
вычисляемых по L и l (по аналогии с методом тяжелого шарика [32]), но в
этом случае можно говорить только о локальной сходимости метода.
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 15Дифференциальные уравнения и процессы управления,N. 4, 2024
3 Вычислительные эксперименты
Рассмотримприменениеразработанногометодакчисленномурешениюзадач
оптимизации,ккоторымсводятсякраевыезадачиизразличныхприложений.
Численные расчеты проводятся при использовании оптимальных параметров
β opt
и h
opt
. Производится сравнение метода (13) со следующими известными
методами, которые тоже будут применяться при значениях параметров, обес-
печивающих наилучшую скорость сходимости:
1) Метод градиентного спуска (2) с оптимальным шагом для сильно вы-
пуклой функции с константой выпуклости l > 0 и липшицевым градиентом
с константой L>0 [32]:
h
opt
=
2
L+l
, ρ opt
=
κ − 1
κ +1
.
2) Метод тяжелого шарика (12) с оптимальными параметрами, получен-
ными для сильно выпуклой квадратичной функции [32]:
h
opt
=
4
(
√
L+
√
l)
2
, β opt
=
 √
κ − 1
√
κ +1
 
2
, ρ opt
=
√
κ − 1
√
κ +1
.
3) Метод Нестерова с оптимальными параметрами, полученными для
класса выпуклых функций с липшицевыми градиентами (метод Nesterov1)
[33]:
x
k+1
=y
k
− h∇f(y
k
), y
k
=x
k
+β (x
k
− x
k− 1
),
h
opt
=
1
L
, β opt
=
√
κ − 1
√
κ +1
, ρ opt
=1− 1
√
κ .
4) Метод Нестерова с оптимальными параметрами, полученными для
квадратичной сильно выпуклой функции (метод Nesterov2) [34]:
h
opt
=
4
3L+l
, β opt
=
√
3κ +1− 2
√
3κ +1+2
, ρ opt
=1− 2
√
3κ +1
.
На рис. 1 представлены графики зависимости ρ opt
от логарифма κ при
κ ≥ 14 в случае γ = 0.29 (при таком значении (16) выполняется для всех
рассматриваемых κ ). Как можно видеть, значения ρ opt
в случае метода (13)
меньше, чем для остальных методов, что должно приводить к его более быст-
рой сходимости на практике. На рис. 2 для сравнения представлены графи-
ки оптимальной скорости сходимости при фиксированном γ и для значений
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 16Дифференциальные уравнения и процессы управления,N. 4, 2024
γ , получаемых при каждом κ из условия (27) при eps = 0.001. Как можно
видеть, при выборе γ в зависимости от κ получается наилучшая скорость
сходимости среди всех рассмотренных методов.
1 2 3 4 5
log
10
( )
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
opt
GD
Nesterov1
Nesterov2
HB
LBHB
Рис.1:Графикизависимостиоптимальнойскоростисходимостиотлогарифма κ дляраз-
личных методов
При решении рассматриваемых ниже задач значение γ выбиралось из
условия (27) при eps = 0.001. Вычисления проводились на ПК следующей
конфигурации: Intel(R) Core(TM) i7-12700k F 3.60 GHz 32 Gb RAM, про-
граммная реализация алгоритмов проводилась в пакете Matlab R2021a. В
качестве тестовых задач были выбраны краевые задачи, возникающие в ма-
тематической физике и вариационном исчислении. Методы спуска являют-
ся одними из наиболее широко используемых и эффективных итерационных
методов решения систем алгебраических уравнений, возникающих при дис-
кретизации таких задач [35, 36].
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 17Дифференциальные уравнения и процессы управления,N. 4, 2024
1 2 3 4 5
log
10
( )
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
opt
HB
LBHB, =0.29
LBHB, = ( )
Рис. 2: Графики зависимости оптимальной скорости сходимости от логарифма κ 3.1 Задача Дирихле для уравнения Пуассона
Рассмотрим уравнение Пуассона в декартовых координатах в области, пред-
ставляющей собой единичный куб:
∂
2
v
∂x
2
+
∂
2
v
∂y
2
+
∂
2
v
∂z
2
=− sin(πy )sin(πz ), x,y,z∈(0,1). (35)
Будем считать, что на границе области Γ поставлены нулевые условия пер-
вого рода: U|
Γ =0. Точное решение этой задачи имеет вид:
v(x,y,z)=
sin(πy )sin(πz )
2π 2
 
1− sinh(
√
2πx )+sinh(
√
2π (1− x))
sinh(
√
2π )
!
.
Построим в области равномерную сетку из N
3
внутренних узлов с шагом
∆ h по всем переменным. Заменим вторые производные в (35) с помощью
симметричных конечных разностей:
∂
2
v
∂x
2
(x
i
,y
j
,z
k
)≈ v
i+1jk
− 2v
ijk
+v
i− 1jk
∆ h
2
,
∂
2
v
∂y
2
(x
i
,y
j
,z
k
)≈ v
ij+1k
− 2v
ijk
+v
ij− 1k
∆ h
2
,
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 18Дифференциальные уравнения и процессы управления,N. 4, 2024
∂
2
v
∂z
2
(x
i
,y
j
,z
k
)≈ v
ijk+1
− 2v
ijk
+v
ijk− 1
∆ h
2
,
гдеv
ijk
≈ v(x
i
,y
j
,z
k
). Подставляя эти приближения в (35), учитывая гранич-
ные условия и производя специальную нумерацию злов (см. [37]), получим
систему линейных алгебраических уравнений:
Au=F,
гдеF (dim(F)=N
3
) — вектор, получаемый по правой части уравнения (35),
u — вектор приближенных значений v в узлах сетки, A — симметричная
положительноопределеннаяблочно-диагональнаяматрица.Еемаксимальное
и минимальное собственные значения вычисляются как [38]:
l =
12
∆ h
2
sin
2
 
π ∆ h
2
 
, L=
12
∆ h
2
cos
2
 
π ∆ h
2
 
.
0 500 1000 1500 2000 2500 3000
k
-4
-3
-2
-1
0
1
2
log
10
(||u
k
- u
*
||)
N=200
GD
Nesterov1
Nesterov2
HB
LBHB
0 1000 2000 3000 4000
k
-4
-3
-2
-1
0
1
2
log
10
(||u
k
- u
*
||)
N=300
GD
Nesterov1
Nesterov2
HB
LBHB
a) b)
Рис.3:Графикизависимостилогарифмапогрешностиотномераитерациипричисленном
решении задачи для (35): a) случай сетки c N =200; b) случай сетки c N =300
При проведении численных расчетов проводилась минимизация квадра-
тичной функции с матрицей A. Для сравнения методов использовалась аб-
солютная погрешность ||u
k
− u
∗ ||
2
, где u
∗ есть точное решение задачи. На
рис. 3 представлены графики зависимости логарифма нормы погрешности
от номера итерации в случаях сеток с N = 200 (κ ≈ 1.637· 10
4
) и N = 300
(κ ≈ 3.671· 10
4
).Какможновидеть,метод(13)сходитсябыстреевсехрассмат-
риваемых методов. В табл. 1 представлено число итераций и время работы в
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 19Дифференциальные уравнения и процессы управления,N. 4, 2024
секундах, нужных для достижения точности 5· 10
− 4
ускоренными методами
спуска. Отметим, что наименьшее число итераций и время тоже имеет место
в случае метода (13).
Таблица 1. Число шагов и время в секундах (в скобках), требуемые для
достижения точности 5· 10
− 4
при применении ускоренных методов к
численному решению задачи для (35)
N κ HB Nesterov1 Nesterov2 LBHB
200 1.637· 10
4
904 1800 1558 454
(89.3) (142) (123) (71.2)
250 2.553· 10
4
1157 2305 1996 581
(175) (363) (314) (151)
300 3.671· 10
4
1414 2821 2442 711
(370) (772) (665) (325)
350 4.993· 10
4
1676 3344 2895 842
(697) (1447) (1250) (635)
400 6.517· 10
4
1942 3857 3355 975
(1221) (2540) (2179) (1119)
3.2 Задача для линейного интегро-дифференциального уравне-
ния
Рассмотрим линейное интегро-дифференциальное уравнение следующего ви-
да:
z
′′
(x)− z
′
(x)− 6z(x)+ε
1
Z
0
z(t)dt=− 2π cos(2πx )− (6+4π 2
)sin(2πx ), (36)
с граничными условиями z(0) = z(1) = 0, при 0 < ε ≪ 1. Точное решение
этой задачи имеет вид:
z
∗ (x)=C
1
 
e
3x
+
ε(e
3
− 1)
3(6− ε)
 
+C
2
 
e
− 2x
+
ε(1− e
− 2
)
2(6− ε)
 
+sin(2πx ),
где
C
1
=
3((1+e
− 2
)ε− 12)
σ (ε)
, C
2
=
2(18+ε(e
3
− 4))
σ (ε)
,
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 20Дифференциальные уравнения и процессы управления,N. 4, 2024
σ (ε)=εe
3
+5ε− 11εe
− 2
+5εe+36e
− 2
− 36e
3
.
Для получения численного решения задачи для (36) построим на проме-
жутке [0,1] сетку с шагом ∆ h и узлами 0 = x
0
< x
1
< ... < x
N
< x
N+1
= 1.
Аппроксимируем производные, входящие в (36), с помощью разностных про-
изводных второго порядка аппроксимации:
z
′′
(x
i
)≈ z
i+1
− 2z
i
+z
i− 1
∆ h
2
, z
′
(x
i
)≈ z
i+1
− z
i− 1
2∆ h
, i=2,N,
z
′′
(x
1
)≈ z
2
− 2z
1
∆ h
2
, z
′′
(x
N
)≈ − 2z
N
+z
N− 1
∆ h
2
, z
′
(x
1
)≈ z
2
2∆ h
, z
′
(x
N
)≈ − z
N− 1
2∆ h
,
где z
i
≈ z(x
i
), а при аппроксимации производных в приграничных узлах
использованы граничные условия.
Дляприближенияинтегралавоспользуемсясоставнойквадратурнойфор-
мулой трапеций:
1
Z
0
z(t)dt≈ ∆ h
2
z(x
0
)+∆ h
N
X
i=1
z(x
i
)+
∆ h
2
z(x
N+1
)≈ ∆ h
N
X
i=1
z
i
.
Подставляя все приближения в (36), получим линейную систему относи-
тельно z
i
:
(A+B+ε∆ h
3
I)z =∆ h
2
b, (37)
где
A=










2 − 1 0 0 ... 0 0 0
− 1 2 − 1 0 ... 0 0 0
0 − 1 2 − 1 ... 0 0 0
... ... ... ... ... ... ... ...
0 0 0 0 ... − 1 2 − 1
0 0 0 0 ... 0 − 1 2










,
B =















− 6∆ h
2
− ∆ h
2
0 0 ... 0 0 0
∆ h
2
− 6∆ h
2
− ∆ h
2
0 ... 0 0 0
0
∆ h
2
− 6∆ h
2
− ∆ h
2
... 0 0 0
... ... ... ... ... ... ... ...
0 0 0 0 ...
∆ h
2
− 6∆ h
2
− ∆ h
2
0 0 0 0 ... 0
∆ h
2
− 6∆ h
2















,
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 21Дифференциальные уравнения и процессы управления,N. 4, 2024
где I — матрица из единиц, а вектор b вычисляется через значения правой
части уравнения (37).
При малых значениях ε и ∆ h можно считать, что A ≈ A+B +ε∆ h
3
I,
в связи с чем в качестве L и l будем рассматривать максимальное и мини-
мальноесобственныезначенияматрицыA,которыевычисляютсяследующим
образом [38]:
l =4sin
2
 
π ∆ h
2
 
, L=4cos
2
 
π ∆ h
2
 
.
Численные расчеты производились при значении ε = 0.01. Погрешность
вычислялась как ||z
k
− z
∗ ||
2
. В качестве начального приближения была вы-
брана функция z
0
(x)=x(1− x).
На рис. 4 представлены графики зависимости логарифма погрешности
от номера итерации для сеток с N = 500 (κ ≈ 1.017· 10
5
) и N = 1000
(κ ≈ 4.061· 10
5
). Как можно заметить, при применении метода (13) нуж-
ная точность достигается быстрее, чем в случаях остальных методов. В табл.
2 представлено число итераций и время работы, необходимые для получе-
ния точности 10
− 6
ускоренными методами в случае разных разбиений сетки.
Наилучшие результаты имеют место для метода (13).
Таблица 2. Число шагов и время в секундах (в скобках), требуемые для
достижения точности 10
− 6
при применении ускоренных методов к
численному решению задачи для (36)
N κ HB Nesterov1 Nesterov2 LBHB
1000 4.061· 10
5
5024 10043 8697 2522
(0.173) (0.296) (0.234) (0.0672)
1500 9.131· 10
5
7548 15092 13070 3789
(0.703) (0.904) (0.845) (0.253)
2000 1.622· 10
6
10077 20149 17449 5058
(2.17) (3.69) (3.36) (0.959)
2500 2.535· 10
6
12609 25214 21836 6330
(7.97) (16.2) (13.9) (4.04)
5000 1.013· 10
7
27524 55038 47664 13817
(123) (247) (213) (62.1)
10000 4.053· 10
7
55566 111128 96240 27896
(1111) (2229) (1928) (557)
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 22Дифференциальные уравнения и процессы управления,N. 4, 2024
a) b)
0 1000 2000 3000 4000 5000
k
-14
-12
-10
-8
-6
-4
-2
0
2
log
10
(||z
k
- z
*
||)
N=500
GD
Nesterov1
Nesterov2
HB
LBHB
0 2000 4000 6000 8000 10000
k
-14
-12
-10
-8
-6
-4
-2
0
2
log
10
(||z
k
- z
*
||)
N=1000
GD
Nesterov1
Nesterov2
HB
LBHB
Рис.4:Графикизависимостилогарифмапогрешностиотномераитерациипричисленном
решении задачи для (36): a) случай сетки c N =500; b) случай сетки c N =1000
3.3 Задача минимизации интегрального функционала
В качестве примера задачи о минимизации функции вида (28) рассмотрим
задачу, возникающую при дискретизации следующей задачи вариационного
исчисления:
I(y)=
1
Z
0
((y
′
(x))
2
− ε(y
′
(x))
4
)dx, ε≪ 1. (38)
где y(x)∈C
1
[0,1], y(0)=y(1)=0.
С учетом того, что подынтегральная функция зависит только от y
′
, урав-
нение Эйлера для этого функционала имеет общее решение y(x)=C
1
x+C
2
[39]. С учетом поставленных граничных условий, экстремаль задачи имеет
вид y
∗ (x)≡ 0.
Для перехода к конечномерной задаче построим на промежутке [0,1] сет-
ку с шагом ∆ h изN внутренних узлов. Интеграл из (38) вычислим с исполь-
зованием составной квадратурной формулы трапеций:
I(y)≈ ∆ h
2
((y
′
(x
0
))
2
− ε(y
′
(x
0
))
4
)+∆ h
N
X
i=1
((y
′
(x
i
))
2
− ε(y
′
(x
i
))
4
)+
∆ h
2
((y
′
(x
N+1
))
2
− ε(y
′
(x
N+1
))
4
). (39)
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 23Дифференциальные уравнения и процессы управления,N. 4, 2024
Аппроксимируем входящие в эту сумму производные с помощью конечных
разностей первого порядка (для аппроксимации y
′
(x
0
) и y
′
(x
N+1
) используем
граничные условия):
y
′
(x
i
)≈ y
i+1
− y
i
∆ h
, i=1,N, y
′
(x
0
)≈ y
1
∆ h
, y
′
(x
N+1
)≈− y
N
∆ h
, (40)
где y
i
≈ y(x
i
).
Подставляя (40) в (39), получим приближение функционала функцией
конечного числа переменных: I(y) ≈ f(y
1
,...,y
N
). Где f представляется
как (28), гдеg отвечает конечномерному приближению члена− ε
1
R
0
(y
′
(x))
4
dx.
Матрица A имеет вид:
A=
1
∆ h










3 − 2 0 0 ... 0 0 0
− 2 4 − 2 0 ... 0 0 0
0 − 2 4 − 2 ... 0 0 0
... ... ... ... ... ... ... ...
0 0 0 0 ... − 2 4 − 2
0 0 0 0 ... 0 − 2 5










.
a) b)
0 2000 4000 6000 8000 10000
k
-25
-20
-15
-10
-5
0
5
log
10
(||y
k
-y
*
|| )
N=1000
GD
Nesterov1
Nesterov2
HB
LBHB
0 0.5 1 1.5 2
k
10
4
-20
-15
-10
-5
0
5
log
10
(||y
k
- y
*
||)
N=2500
GD
Nesterov1
Nesterov2
HB
LBHB
Рис.5:Графикизависимостилогарифмапогрешностиотномераитерациипричисленном
решении задачи о минимизации (38): a) случай сетки c N = 1000; b) ) случай сетки c
N =2500
Эффект малого возмущения реализуется за чем малого параметра ε и
шага сетки. При проведении численных расчетов на различных сетках ис-
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 24Дифференциальные уравнения и процессы управления,N. 4, 2024
пользовалось значение ε = 0.01. При таком выборе собственные значения
матрицы A доминируют в спектре ∇
2
f(y). Начальное приближение вычис-
лялось как y
0
(x) = x(1− x), l и L находились численно с использованием
Matlab. Погрешность вычислялась как||y
k
− y
∗ ||
2
.
На рис. 5 представлены графики зависимости логарифма погрешности
от номера итерации для сеток с N = 1000 (κ ≈ 4.066· 10
5
) и N = 2500
(κ ≈ 2.536· 10
6
). Как можно заметить, метод (13) сходится быстрее, чем
остальные методы. В табл. 3 представлено число итераций и время работы,
необходимые для получения точности 10
− 6
ускоренными методами в случае
разных разбиений сетки. Как можно видеть, наилучшие результаты имеют
место для метода (13).
Таблица 3. Число шагов и время в секундах (в скобках), требуемые для
достижения точности 10
− 6
при применении ускоренных методов к
численному решению задачи о минимизации (38)
N κ HB Nesterov1 Nesterov2 LBHB
500 1.017· 10
5
2262 4523 3917 1137
(14.1) (27.9) (25.3) (7.21)
1000 4.061· 10
5
4546 9090 7872 2238
(74.4) (149) (131) (39.1)
1500 9.131· 10
5
6836 13670 11838 3433
(209) (420) (362) (108)
2000 1.622· 10
6
9129 18257 15811 4584
(463) (922) (803) (206)
2500 2.535· 10
6
11425 22849 19788 5737
(858) (1706) (1478) (445)
3.4 Задача для нелинейного интегро-дифференциального уравне-
ния
Рассмотрим нелинейное интегро-дифференциальное уравнение [3]:
u
′′
(x)=
1
Z
0
u
4
(s)ds
(1+|x− s|)
2
, x∈(0,1), (41)
с граничными условями u(0) = 1, u(1) = 0. Такая модель описывает стацио-
нарное распределение температуры с учетом нелокальных эффектов [40].
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 25Дифференциальные уравнения и процессы управления,N. 4, 2024
При численном решении задачи построим на [0,1] равномерную сетку,
как в предыдущих примерах. Конечномерная задача о нахожденииu
i
≈ u(x
i
)
получается следующим образом: вторая производная в (41) аппроксимирует-
ся с помощью второй разностной производной, а для вычисления интеграла
при каждомx
i
используем составную формулу трапеций с учетом граничных
условий:
1
Z
0
u
4
(s)ds
(1+|x− s|)
2
≈ ∆ h
2(1+i∆ h)
2
+
N
X
j=1
u
4
j
∆ h
(1+∆ h|i− j|)
2
. (42)
После дискретизации (41) получим систему вида:
Au=−∇ g(u), (43)
где∇g соответствует правой части (42), а A есть симметричная положитель-
ная определенная матрица, отвечающая дискретизации второй производной.
Ее минимальное и максимальное собтвенные значения вычисляются как [38]:
l =
4
∆ h
2
sin
2
 
π ∆ h
2
 
, L=
4
∆ h
2
cos
2
 
π ∆ h
2
 
.
a) b)
0 2000 4000 6000 8000
k
-14
-12
-10
-8
-6
-4
-2
0
2
log
10
(||u
k
- u
*
||)
N=500
GD
Nesterov1
Nesterov2
HB
LBHB
0 2000 4000 6000 8000 10000 12000
k
-14
-12
-10
-8
-6
-4
-2
0
2
log
10
(||u
k
- u
*
||)
N=1000
GD
Nesterov1
Nesterov2
HB
LBHB
Рис.6:Графикизависимостилогарифмапогрешностиотномераитерациипричисленном
решениикраевойзадачидля(41):a)случайсеткиc N =500;b))случайсеткиcN =1000
Систему (43) можно трактовать как необходимое условие минимума воз-
мущенной квадратичной функции f(u):
Au+∇g(u)=0⇔∇f(u)=0.
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 26Дифференциальные уравнения и процессы управления,N. 4, 2024
Таким образом, градиент f известен, и для численного решения (43) мож-
но применять градиентные методы. Погрешность вычислялась следующим
образом: ||u
k
− u
∗ ||
2
, где решение u
∗ для каждого разбиения сетки находи-
лось посредством применения метода (13) при 5· 10
4
итерациях. Начальное
приближение вычислялось следующим образом: u
0
(x)=1− x
2
.
На рис. 6 представлены графики зависимости логарифма погрешности
от номера итерации в случае сеток с N = 500 (κ ≈ 1.017· 10
5
) и N = 1000
(κ ≈ 4.061· 10
5
). Как можно видеть, для этой задачи метод (13) тоже де-
монстрирует лучшую скорость сходимости по сравнению с другими методам.
В табл. 4 представлено число итераций и время, необходимое для достиже-
ния точности 10
− 6
при применении ускоренных методов. Наименьшее число
шагов и время характерны для метода (13).
Таблица 4. Число шагов и время в секундах (в скобках), требуемые для
достижения точности 10
− 6
при применении ускоренных методов к
численному решению задачи для (41)
N κ HB Nesterov1 Nesterov2 LBHB
200 1.637· 10
4
904 1800 1558 454
(89.3) (142) (123) (71.2)
250 2.553· 10
4
1157 2305 1996 581
(175) (363) (314) (151)
300 3.671· 10
4
1414 2821 2442 711
(370) (772) (665) (325)
350 4.993· 10
4
1676 3344 2895 842
(697) (1447) (1250) (635)
400 6.517· 10
4
1942 3857 3355 975
(1221) (2540) (2179) (1119)
4 Заключение
Внастоящейработепредложенметодградиентногоспускасинерциейдляре-
шения задачи минимизации выпуклой функции, основанный на применении
модифицированного МРК второго порядка, при построении которого исполь-
зуется разложение ЛБ. Для сильно выпуклой квадратичной и возмущенной
квадратичной функций доказаны теоремы о сходимости и получены анали-
тические выражения для оптимальных параметров, при которых скорость
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 27Дифференциальные уравнения и процессы управления,N. 4, 2024
сходимости является наилучшей. В квадратичном случае для широкого диа-
назона значений числа обусловленности κ показано, что скорость сходимости
метода лучше, чем для ряда других известных методов. Рассмотрено при-
менение метода к численному решению задачи для трехмерного уравнения
Пуассона,задачдляинтегро-дифференциальныхуравнений,задачивариаци-
онного исчисления. Показано, что предложенный метод сходится за меньшее
число итераций и время по сравнению с другими известными градиентными
методами.
Список литературы
[1] Ascher U. M., van den Doel K., Huang H., Svaiter B. F. Gradient descent and
fast artificial time integration// ESAIM: M2AN. 2009, vol. 43. P. 689–708.
[2] Porta F., Cornelio A., Ruggiero V. Runge–Kutta-like scaling techniques
for first-order methods in convex optimization // Applied Numerical
Mathematics. 2017, vol. 116. P. 256–272.
[3] Eftekhari A. , Vandereycken B., Vilmart G., Zygalakis K. C. Explicit
stabilised gradient descent for faster strongly convex optimisation // BIT
Numerical Mathematics. 2021, vol. 61. P. 119–139.
[4] Stillfjord T. , Williamson M. SRKCD: A stabilized Runge–Kutta method
for stochastic optimization // Journal of Computational and Applied
Mathematics. 2023, vol. 417. Art. no. 114575.
[5] Zhang J., Mokhtari A., Sra S., Jadbabaie A. Direct Runge-Kutta
discretisation achieves acceleration // Advances in Neural Information
Processing Systems. 2018, vol. 31.
[6] Zhang J., Sra S., Jadbabaie A. Acceleration in first order quasi-strongly
convex optimization by ode discretization // 2019 IEEE 58th Conference
on Decision and Control. 2019.
[7] Shi B., Du S.S., Jordan M.I., Su W.J. Understanding the acceleration
phenomenon via high-resolution differential equations // Mathematical
Programming. 2022, vol. 195. P. 79–148.
[8] Luo H., Chen L. From differential equation solvers to accelerated first-
order methods for convex optimization// Mathematical Programming. 2022,
vol. 195. P. 735–781.
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 28Дифференциальные уравнения и процессы управления,N. 4, 2024
[9] Duruisseaux V., Leok M. Practical perspectives on symplectic accelerated
optimization // Optimization Methods and Software. 2023, vol. 38, no. 6.
P. 1230–1268.
[10] Chen R., Li X. Implicit Runge-Kutta methods for accelerated unconstrained
convex optimization // IEEE Access. 2020, vol. 8. P. 28624–28634.
[11] Areias P., Rabczuk T. An engineering interpretation of Nesterov’s convex
minimization algorithm and time integration: application to optimal fiber
orientation// Computational Mechanics. 2021, vol. 68, no. 1. P. 211–227.
[12] Альбер С. И., Альбер Я. И. Применение метода дифференциального
спуска для решения нелинейных систем // Журнал вычислительной ма-
тематики и математической физики. 1967, т. 68, № 1. С. 14–32.
[13] Abbott J. P., Brent R. P. Fast local convergence with single and
multistep methods for nonlinear equations // The Journal of the Australian
Mathematical Society. Series B. Applied Mathematics. 1977, vol. 20, no. 2.
P. 173–199.
[14] Brown A. A., Bartholomew-Biggs M. C. Some effective methods for
unconstrained optimization based on the solution of systems of ordinary
differential equations // Journal of Optimization Theory and Applications.
1989, vol. 62, no. 2. P. 211–224.
[15] Khiyabani F. M., Leong W. J. Quasi-Newton methods based on ordinary
differential equation approach for unconstrained nonlinear optimization //
Applied Mathematics and Computation. 2014, vol. 233. P. 272–291.
[16] Su W., Boyd S., Candes E. J. A differential equation for modeling Nesterov’s
accelerated gradient method: Theory and insights // Journal of Machine
Learning Research. 2016, vol. 17, no. 53. P. 1–43.
[17] Shi B., Du S. S., Su W., Jordan M. I. Acceleration via symplectic
discretization of high-resolution differential equations // Advances in Neural
Information Processing Systems. 2019, vol. 17, no. 32.
[18] ChanR.P.K.,TsaiA.Y. J.Onexplicittwo-derivativeRunge-Kuttamethods
// Numerical Algorithms. 2010, vol. 53, no. 1. P. 171–194.
[19] Turaci M. O., Ozis T. Derivation of three-derivative Runge-Kutta methods
// Numerical Algorithms. 2016, vol. 74. P. 247–265.
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 29Дифференциальные уравнения и процессы управления,N. 4, 2024
[20] Qin X., Yu J., Yan C. Derivation of three-derivative two-step Runge–Kutta
methods // Mathematics. 2024, vol. 12, no. 5. Art. no. 711.
[21] DangQ.A.,HoangM.T.Positiveandelementarystableexplicitnonstandard
Runge–Kutta methods for a class of autonomous dynamical systems //
International Journal of Computer Mathematics. 2020, vol. 97, no. 10.
P. 2036–2054.
[22] Арушанян О. Б. , Залеткин С. Ф. Приближенное решение задачи Коши
для обыкновенных дифференциальных уравнений методом рядов Чебы-
шeва // Вычислительные методы и программирование. 2016, т. 17, № 2.
С. 121–131.
[23] Ворожцов Е. В. Построение явных разностных схем для обыкновен-
ных дифференциальных уравнений с помощью разложений Лагранжа —
Бюрмана // Вычислительные методы и программирование. 2010, т. 11,
№ 2. С. 198–209.
[24] Vorozhtsov E. V. Derivation of explicit difference schemes for ordinary
differential equations with the aid of Lagrange–Burmann expansions //
Lecture Notes in Computer Science. 2010, vol. 6244. P. 250–266.
[25] Ворожцов Е. В. Применение разложений Лагранжа – Бюрмана для чис-
ленного интегрирования уравнений невязкого газа // Вычислительные
методы и программирование. 2011, т. 12, № 3. С. 348–361.
[26] Ворожцов Е. В. Конструирование схем третьего порядка точности с по-
мощью разложений Лагранжа – Бюрмана для численного интегрирова-
ние уравнений невязкого газа // Вычислительные методы и программи-
рование. 2016, т. 17, № 1. С. 21–43.
[27] Jerez S. Non-standard Lagrange–Burman methods for the numerical
integration of differential equations // Journal of Difference Equations and
Applications. 2012, vol. 18, no. 11. P. 1899–1912.
[28] Поляк Б. Т. О некоторых способах ускорения сходимости итерационных
методов // Журнал вычислительной математики и математической фи-
зики. 1964, т. 4, № 5. С. 791–803.
[29] Абрамовиц М., Стиган И. Справочник по специальным функциям. М.:
Наука, 1979. 832 с.
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 30Дифференциальные уравнения и процессы управления,N. 4, 2024
[30] Бахвалов Н. С., Жидков Н. П., Кобельков Г. М. Численные методы. М.:
Лаборатория знаний, 2020. 636 с.
[31] Гантмахер Ф. Р. Теория матриц. М.: ФИЗМАТЛИТ, 2010. 560 с.
[32] Поляк Б. Т. Введение в оптимизацию. М.: Наука, 1983. 384 с.
[33] Нестеров Ю. Е. Введение в выпуклую оптимизацию. М.: МЦНМО, 2010.
280 с.
[34] Lessard L., Recht B., Packard A. Analysis and design of optimization
algorithms via integral quadratic constraints // SIAM Journal on
Optimization. 2016, vol. 26, no. 1. P. 57–95.
[35] Самарский А. А., Николаев Е. С. Методы решения сеточных уравнений.
М.: Наука, 1978. 592 с.
[36] Николаев Е. С. Методы решения сеточных уравнений. М.: Изд-во МГУ,
2023. 404 с.
[37] Ортега Д., Пул У. Введение в численные методы решения дифференци-
альных уравнений. М.: Наука, 1986. 288 с.
[38] Самарский А. А., Гулин А. В. Устойчивость разностных схем. М.: Наука,
1973. 415 с.
[39] Эльсгольц Л. Э. Вариационное исчисление. М.: УРСС, 2023. 208 с.
[40] Vasudeva M. A., Verwer J. Solving parabolic integro-differential equations
by an explicit integration method // Journal of Computational and Applied
Mathematics. 1992, vol. 39, no. 1. P. 121–132.
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 31Дифференциальные уравнения и процессы управления,N. 4, 2024
Application of the modified Runge–Kutta method to the construction
of the descent method for solving boundary value problems
G.V. Krivovichev, N.V. Egorov
Saint-Petersburg State University, Faculty of Applied Mathematics and Control
Processes
E-mail: g.krivovichev@spbu.ru, n.v.egorov@spbu.ru
Abstract. The work is devoted to the construction and analysis of the
gradient method based on a modified explicit second-order Runge–Kutta method,
constructed using the Lagrange–Burmann expansion. A two-step method with
inertia based on the heavy ball method is proposed. Convergence theorems
are proven for strongly convex quadratic and perturbed quadratic functions.
Analytical expressions for the optimal parameters of the method are obtained. For
the quadratic function it is demonstrated, that the proposed method converges
faster, than other well-known accelerated methods.
The results of the application of the method to the numerical solution
of linear and nonlinear boundary value problems (Dirichlet problem for a 3D
Poisson equation, problems from the calculus of variations, problems for integro-
differential equations) are presented. It is demonstrated that, in comparison with
well-known methods, the proposed method allows one to obtain a numerical
solution with the required accuracy for different grid resolutions in a smaller
number of iterations and time.
Key words: Runge–Kutta methods, convex optimization, gradient descent,
boundary value problems
https://doi.org/10.21638/11701/spbu35.2024.401Электронныйжурнал: http://diffjournal.spbu.ru/ 32
